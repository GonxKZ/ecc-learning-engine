#include "../framework/ecscope_test_framework.hpp"
#include <ecscope/ecs_performance_benchmarker.hpp>
#include <ecscope/ecs_performance_regression_tester.hpp>
#include <ecscope/ecs_performance_visualizer.hpp>
#include <ecscope/performance_lab.hpp>
#include <ecscope/memory_benchmark_suite.hpp>
#include <ecscope/benchmarks.hpp>

#ifdef ECSCOPE_ENABLE_PHYSICS
#include <ecscope/advanced_physics_benchmarks.hpp>
#endif

#include <ecscope/audio_systems.hpp>
#include <ecscope/networking/ecs_networking_system.hpp>

#include <fstream>
#include <thread>
#include <future>
#include <random>
#include <algorithm>

namespace ECScope::Testing {

using namespace ecscope::performance;
using namespace ecscope::performance::ecs;

class ComprehensivePerformanceTest : public PerformanceTestFixture {
protected:
    void SetUp() override {
        PerformanceTestFixture::SetUp();
        
        // Initialize performance tracking infrastructure
        performance_memory_tracker_ = std::make_unique<MemoryTracker>("PerformanceTests");
        performance_memory_tracker_->start_tracking();
        
        // Initialize performance lab
        PerformanceLabConfig lab_config;
        lab_config.enable_real_time_monitoring = true;
        lab_config.enable_regression_detection = true;
        lab_config.enable_memory_profiling = true;
        lab_config.enable_cache_analysis = true;
        
        performance_lab_ = std::make_unique<PerformanceLab>(lab_config);
        
        // Initialize ECS performance benchmarker
        ecs_benchmarker_ = std::make_unique<ECSPerformanceBenchmarker>(
            ECSBenchmarkConfig::create_comprehensive());
        
        // Initialize regression tester
        regression_tester_ = std::make_unique<ECSPerformanceRegressionTester>();
        
        // Initialize memory benchmark suite
        memory_benchmarker_ = std::make_unique<memory::MemoryBenchmarkSuite>();
        
        // Initialize performance visualizer
        performance_visualizer_ = std::make_unique<ECSPerformanceVisualizer>();
        
        // Load baseline performance data if available
        load_baseline_performance_data();
        
        // Create test scenarios
        setup_performance_test_scenarios();
    }
    
    void TearDown() override {
        // Save performance results for future regression testing
        save_performance_results();
        
        // Generate comprehensive performance report
        generate_final_performance_report();
        
        // Clean up performance systems
        performance_visualizer_.reset();
        memory_benchmarker_.reset();
        regression_tester_.reset();
        ecs_benchmarker_.reset();
        performance_lab_.reset();
        
        // Validate no performance-related memory leaks
        if (performance_memory_tracker_) {
            performance_memory_tracker_->stop_tracking();
            EXPECT_EQ(performance_memory_tracker_->get_allocation_count(),
                     performance_memory_tracker_->get_deallocation_count());
        }
        
        performance_memory_tracker_.reset();
        
        PerformanceTestFixture::TearDown();
    }
    
    void setup_performance_test_scenarios() {
        // Scenario 1: ECS Scaling Performance
        test_scenarios_.emplace_back(PerformanceTestScenario{
            "ECS_Scaling",
            "Test ECS performance scaling with entity count",
            [this]() { return test_ecs_scaling_performance(); },
            30.0, // 30 second timeout
            {"ECS", "Scaling", "Memory", "Cache"}
        });
        
        // Scenario 2: Memory Allocation Patterns
        test_scenarios_.emplace_back(PerformanceTestScenario{
            "Memory_Patterns",
            "Test various memory allocation patterns",
            [this]() { return test_memory_allocation_patterns(); },
            45.0,
            {"Memory", "Allocation", "Fragmentation", "Pools"}
        });
        
        // Scenario 3: Multi-System Integration
        test_scenarios_.emplace_back(PerformanceTestScenario{
            "Multi_System_Integration",
            "Test performance with multiple systems running",
            [this]() { return test_multi_system_integration(); },
            60.0,
            {"Integration", "Physics", "Audio", "Networking"}
        });
        
        // Scenario 4: Real-time Constraints
        test_scenarios_.emplace_back(PerformanceTestScenario{
            "Real_Time_Constraints",
            "Validate real-time performance constraints",
            [this]() { return test_real_time_constraints(); },
            120.0,
            {"Real-time", "Frame-rate", "Jitter", "Latency"}
        });
        
        // Scenario 5: Stress Testing
        test_scenarios_.emplace_back(PerformanceTestScenario{
            "Stress_Testing",
            "High-load stress testing scenarios",
            [this]() { return test_stress_scenarios(); },
            180.0,
            {"Stress", "Load", "Stability", "Degradation"}
        });
    }
    
    void load_baseline_performance_data() {
        std::ifstream baseline_file("performance_baseline.json");
        if (baseline_file.is_open()) {
            // Load existing baseline data
            std::string baseline_json((std::istreambuf_iterator<char>(baseline_file)),
                                     std::istreambuf_iterator<char>());
            baseline_file.close();
            
            baseline_results_ = parse_performance_results_json(baseline_json);
            has_baseline_data_ = true;
            
            std::cout << "Loaded " << baseline_results_.size() 
                      << " baseline performance results\n";
        } else {
            std::cout << "No baseline performance data found, creating new baseline\n";
            has_baseline_data_ = false;
        }
    }
    
    void save_performance_results() {
        if (!current_results_.empty()) {
            std::string results_json = serialize_performance_results_json(current_results_);
            
            std::ofstream results_file("performance_results_" + get_timestamp() + ".json");
            if (results_file.is_open()) {
                results_file << results_json;
                results_file.close();
            }
            
            // Update baseline if this is a new baseline run
            if (!has_baseline_data_) {
                std::ofstream baseline_file("performance_baseline.json");
                if (baseline_file.is_open()) {
                    baseline_file << results_json;
                    baseline_file.close();
                }
            }
        }
    }
    
    void generate_final_performance_report() {
        std::ofstream report("comprehensive_performance_report.md");
        if (!report.is_open()) return;
        
        report << "# ECScope Comprehensive Performance Test Report\\n\\n";\n        report << "Generated: " << get_timestamp() << "\\n\\n";\n        \n        report << "## Test Environment\\n";\n        report << "- Platform: " << get_platform_info() << "\\n";\n        report << "- Compiler: " << get_compiler_info() << "\\n";\n        report << "- Build Configuration: " << get_build_config() << "\\n\\n";\n        \n        report << "## Performance Test Summary\\n";\n        report << "- Total Scenarios: " << test_scenarios_.size() << "\\n";\n        report << "- Completed Tests: " << completed_scenarios_ << "\\n";\n        report << "- Failed Tests: " << failed_scenarios_ << "\\n";\n        report << "- Total Test Time: " << total_test_time_seconds_ << " seconds\\n\\n";\n        \n        // ECS Performance Results\n        if (ecs_benchmarker_) {\n            report << "## ECS Performance Results\\n";\n            auto ecs_results = ecs_benchmarker_->get_results();\n            \n            for (const auto& result : ecs_results) {\n                report << "### " << result.test_name << " (" \n                       << ECSPerformanceBenchmarker::architecture_to_string(result.architecture_type) \n                       << ")\\n";\n                report << "- Entities: " << result.entity_count << "\\n";\n                report << "- Average Time: " << result.average_time_us << " Î¼s\\n";\n                report << "- Throughput: " << result.entities_per_second << " entities/sec\\n";\n                report << "- Memory Usage: " << result.peak_memory_usage << " bytes\\n";\n                report << "- Cache Hit Ratio: " << (result.cache_hit_ratio * 100.0) << "%\\n\\n";\n            }\n        }\n        \n        // Memory Performance Results\n        if (memory_benchmarker_) {\n            report << "## Memory Performance Results\\n";\n            auto memory_results = memory_benchmarker_->get_comprehensive_results();\n            \n            report << "- Peak Memory Usage: " << memory_results.peak_memory_usage << " bytes\\n";\n            report << "- Total Allocations: " << memory_results.total_allocations << "\\n";\n            report << "- Average Allocation Size: " << memory_results.average_allocation_size << " bytes\\n";\n            report << "- Fragmentation Ratio: " << memory_results.fragmentation_ratio << "\\n";\n            report << "- Memory Bandwidth: " << memory_results.memory_bandwidth_mb_per_sec << " MB/s\\n\\n";\n        }\n        \n        // Regression Analysis\n        if (has_baseline_data_ && regression_tester_) {\n            report << "## Regression Analysis\\n";\n            auto regression_report = regression_tester_->detect_regressions(current_results_);\n            \n            if (!regression_report.regressions.empty()) {\n                report << "### Performance Regressions Detected\\n";\n                for (const auto& regression : regression_report.regressions) {\n                    report << "- " << regression.test_name << ": "\n                           << (regression.performance_change * 100.0) << "% slower\\n";\n                }\n            } else {\n                report << "No performance regressions detected.\\n";\n            }\n            \n            if (!regression_report.improvements.empty()) {\n                report << "### Performance Improvements\\n";\n                for (const auto& improvement : regression_report.improvements) {\n                    report << "- " << improvement.test_name << ": "\n                           << (improvement.performance_change * 100.0) << "% faster\\n";\n                }\n            }\n        }\n        \n        report << "\\n## Recommendations\\n";\n        if (ecs_benchmarker_) {\n            report << ecs_benchmarker_->generate_optimization_recommendations() << "\\n";\n        }\n        \n        report.close();\n    }\n    \n    // Performance test scenario implementations\n    bool test_ecs_scaling_performance() {\n        ecs_benchmarker_->register_all_standard_tests();\n        \n        std::vector<u32> entity_counts = {100, 500, 1000, 5000, 10000, 25000};\n        ecs_benchmarker_->run_scaling_analysis(entity_counts);\n        \n        // Wait for completion with timeout\n        auto timeout = std::chrono::steady_clock::now() + std::chrono::seconds(30);\n        while (ecs_benchmarker_->is_running() && \n               std::chrono::steady_clock::now() < timeout) {\n            std::this_thread::sleep_for(std::chrono::milliseconds(100));\n        }\n        \n        if (ecs_benchmarker_->is_running()) {\n            ecs_benchmarker_->cancel_benchmarks();\n            return false;\n        }\n        \n        auto results = ecs_benchmarker_->get_results();\n        current_results_.insert(current_results_.end(), results.begin(), results.end());\n        \n        return !results.empty();\n    }\n    \n    bool test_memory_allocation_patterns() {\n        memory_benchmarker_->configure_comprehensive_testing();\n        memory_benchmarker_->run_all_benchmarks();\n        \n        // Test various allocation patterns\n        std::vector<std::string> pattern_tests = {\n            "SequentialAllocation",\n            "RandomAllocation", \n            "PoolAllocation",\n            "ArenaAllocation",\n            "FragmentationTest"\n        };\n        \n        for (const auto& test : pattern_tests) {\n            memory_benchmarker_->run_specific_test(test);\n        }\n        \n        auto memory_results = memory_benchmarker_->get_results();\n        return !memory_results.empty();\n    }\n    \n    bool test_multi_system_integration() {\n        // Create integrated system scenario\n        constexpr int entity_count = 1000;\n        std::vector<Entity> entities;\n        \n        // Create entities with multiple system components\n        for (int i = 0; i < entity_count; ++i) {\n            auto entity = world_->create_entity();\n            \n            // Add transform\n            Vec3 position = {\n                static_cast<f32>(i % 20) * 5.0f - 50.0f,\n                0.0f,\n                static_cast<f32>(i / 20) * 5.0f - 25.0f\n            };\n            world_->add_component<Transform>(entity, position);\n            \n            // Add test components for ECS\n            world_->add_component<TestPosition>(entity, position.x, position.y, position.z);\n            world_->add_component<TestVelocity>(entity, \n                static_cast<f32>(i % 5 - 2), \n                0.0f, \n                static_cast<f32>(i % 3 - 1));\n            \n#ifdef ECSCOPE_ENABLE_PHYSICS\n            // Add physics components\n            if (i % 2 == 0) {\n                Physics::RigidBody rigidbody;\n                rigidbody.mass = 1.0f;\n                world_->add_component<Physics::RigidBody>(entity, rigidbody);\n                \n                Physics::SphereCollider collider;\n                collider.radius = 0.5f;\n                world_->add_component<Physics::SphereCollider>(entity, collider);\n            }\n#endif\n            \n            // Add audio components\n            if (i % 3 == 0) {\n                audio::AudioSource audio_source;\n                audio_source.volume = 0.1f;\n                audio_source.enable_hrtf = (i % 6 == 0);\n                world_->add_component<audio::AudioSource>(entity, audio_source);\n            }\n            \n            entities.push_back(entity);\n        }\n        \n        // Create audio listener\n        auto listener = world_->create_entity();\n        world_->add_component<Transform>(listener, Vec3{0.0f, 0.0f, 0.0f});\n        audio::AudioListener audio_listener;\n        audio_listener.is_active = true;\n        world_->add_component<audio::AudioListener>(listener, audio_listener);\n        \n        // Performance benchmark of integrated systems\n        auto start_time = std::chrono::high_resolution_clock::now();\n        \n        for (int frame = 0; frame < 300; ++frame) { // 10 seconds at 30 FPS\n            world_->update(0.033f);\n            \n            // Add some dynamic behavior\n            if (frame % 30 == 0) {\n                for (int i = 0; i < 10; ++i) {\n                    auto entity = entities[i + (frame / 30) * 10];\n                    auto& velocity = world_->get_component<TestVelocity>(entity);\n                    velocity.vx = static_cast<f32>((frame % 7) - 3);\n                    velocity.vz = static_cast<f32>((frame % 5) - 2);\n                }\n            }\n        }\n        \n        auto end_time = std::chrono::high_resolution_clock::now();\n        auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end_time - start_time);\n        \n        // Record performance result\n        ECSBenchmarkResult integration_result;\n        integration_result.test_name = "MultiSystemIntegration";\n        integration_result.category = ECSBenchmarkCategory::Integration;\n        integration_result.entity_count = entity_count;\n        integration_result.average_time_us = duration.count() * 1000.0 / 300.0; // Convert to microseconds per frame\n        integration_result.is_valid = true;\n        \n        current_results_.push_back(integration_result);\n        \n        // Clean up\n        for (auto entity : entities) {\n            world_->destroy_entity(entity);\n        }\n        world_->destroy_entity(listener);\n        \n        return duration.count() < 15000; // Should complete in under 15 seconds\n    }\n    \n    bool test_real_time_constraints() {\n        // Test real-time performance constraints (60 FPS)\n        constexpr int target_fps = 60;\n        constexpr f32 target_frame_time_ms = 1000.0f / target_fps;\n        constexpr int test_frames = 300; // 5 seconds\n        \n        // Create moderate load scenario\n        std::vector<Entity> entities;\n        for (int i = 0; i < 200; ++i) {\n            auto entity = create_test_entity_with_components();\n            entities.push_back(entity);\n        }\n        \n        std::vector<f32> frame_times;\n        frame_times.reserve(test_frames);\n        \n        // Measure frame times\n        for (int frame = 0; frame < test_frames; ++frame) {\n            auto frame_start = std::chrono::high_resolution_clock::now();\n            \n            world_->update(1.0f / target_fps);\n            \n            auto frame_end = std::chrono::high_resolution_clock::now();\n            auto frame_duration = std::chrono::duration_cast<std::chrono::microseconds>(\n                frame_end - frame_start);\n            \n            f32 frame_time_ms = frame_duration.count() / 1000.0f;\n            frame_times.push_back(frame_time_ms);\n        }\n        \n        // Analyze frame time statistics\n        f32 average_frame_time = std::accumulate(frame_times.begin(), frame_times.end(), 0.0f) / frame_times.size();\n        f32 max_frame_time = *std::max_element(frame_times.begin(), frame_times.end());\n        \n        std::sort(frame_times.begin(), frame_times.end());\n        f32 percentile_95 = frame_times[static_cast<size_t>(frame_times.size() * 0.95)];\n        f32 percentile_99 = frame_times[static_cast<size_t>(frame_times.size() * 0.99)];\n        \n        // Count missed frames\n        int missed_frames = std::count_if(frame_times.begin(), frame_times.end(),\n            [target_frame_time_ms](f32 time) { return time > target_frame_time_ms; });\n        \n        f32 missed_frame_percentage = (static_cast<f32>(missed_frames) / test_frames) * 100.0f;\n        \n        // Record real-time performance result\n        ECSBenchmarkResult real_time_result;\n        real_time_result.test_name = "RealTimeConstraints";\n        real_time_result.category = ECSBenchmarkCategory::Stress;\n        real_time_result.entity_count = entities.size();\n        real_time_result.average_time_us = average_frame_time * 1000.0;\n        real_time_result.max_time_us = max_frame_time * 1000.0;\n        real_time_result.consistency_score = 1.0 - (missed_frame_percentage / 100.0);\n        real_time_result.is_valid = true;\n        \n        current_results_.push_back(real_time_result);\n        \n        // Clean up\n        for (auto entity : entities) {\n            world_->destroy_entity(entity);\n        }\n        \n        // Real-time constraints: average < target, 95% < 1.5x target, < 5% missed frames\n        return average_frame_time < target_frame_time_ms &&\n               percentile_95 < target_frame_time_ms * 1.5f &&\n               missed_frame_percentage < 5.0f;\n    }\n    \n    bool test_stress_scenarios() {\n        // High entity count stress test\n        constexpr int stress_entity_count = 10000;\n        std::vector<Entity> entities;\n        \n        auto start_time = std::chrono::high_resolution_clock::now();\n        \n        // Create many entities\n        for (int i = 0; i < stress_entity_count; ++i) {\n            auto entity = create_test_entity_with_components();\n            entities.push_back(entity);\n        }\n        \n        auto creation_time = std::chrono::high_resolution_clock::now();\n        \n        // Update systems under stress\n        for (int frame = 0; frame < 60; ++frame) { // 2 seconds at 30 FPS\n            world_->update(0.033f);\n        }\n        \n        auto update_time = std::chrono::high_resolution_clock::now();\n        \n        // Destroy entities\n        for (auto entity : entities) {\n            world_->destroy_entity(entity);\n        }\n        \n        auto end_time = std::chrono::high_resolution_clock::now();\n        \n        // Calculate timings\n        auto creation_duration = std::chrono::duration_cast<std::chrono::milliseconds>(creation_time - start_time);\n        auto update_duration = std::chrono::duration_cast<std::chrono::milliseconds>(update_time - creation_time);\n        auto destruction_duration = std::chrono::duration_cast<std::chrono::milliseconds>(end_time - update_time);\n        \n        // Record stress test results\n        ECSBenchmarkResult stress_result;\n        stress_result.test_name = "StressTest";\n        stress_result.category = ECSBenchmarkCategory::Stress;\n        stress_result.entity_count = stress_entity_count;\n        stress_result.average_time_us = (creation_duration.count() + update_duration.count() + destruction_duration.count()) * 1000.0;\n        stress_result.scalability_factor = static_cast<f64>(stress_entity_count) / stress_result.average_time_us;\n        stress_result.is_valid = true;\n        \n        current_results_.push_back(stress_result);\n        \n        std::cout << "Stress Test Results:\\n"\n                  << "  Entity Creation: " << creation_duration.count() << "ms\\n"\n                  << "  Update Phase: " << update_duration.count() << "ms\\n"\n                  << "  Entity Destruction: " << destruction_duration.count() << "ms\\n";\n        \n        // Stress test passes if it completes within reasonable time (< 30 seconds total)\n        auto total_duration = std::chrono::duration_cast<std::chrono::seconds>(end_time - start_time);\n        return total_duration.count() < 30;\n    }\n    \n    Entity create_test_entity_with_components() {\n        auto entity = world_->create_entity();\n        \n        // Add basic components\n        world_->add_component<TestPosition>(entity, 0.0f, 0.0f, 0.0f);\n        world_->add_component<TestVelocity>(entity, 1.0f, 0.0f, 0.0f);\n        world_->add_component<TestHealth>(entity, 100, 100);\n        \n        return entity;\n    }\n    \n    std::string get_timestamp() {\n        auto now = std::chrono::system_clock::now();\n        auto time_t = std::chrono::system_clock::to_time_t(now);\n        std::stringstream ss;\n        ss << std::put_time(std::localtime(&time_t), "%Y%m%d_%H%M%S");\n        return ss.str();\n    }\n    \n    std::string get_platform_info() {\n        return "Test Platform"; // TODO: Implement actual platform detection\n    }\n    \n    std::string get_compiler_info() {\n        return "Test Compiler"; // TODO: Implement actual compiler detection\n    }\n    \n    std::string get_build_config() {\n#ifdef NDEBUG\n        return "Release";\n#else\n        return "Debug";\n#endif\n    }\n    \n    std::vector<ECSBenchmarkResult> parse_performance_results_json(const std::string& json) {\n        // TODO: Implement JSON parsing for baseline results\n        return {};\n    }\n    \n    std::string serialize_performance_results_json(const std::vector<ECSBenchmarkResult>& results) {\n        // TODO: Implement JSON serialization for results\n        return "{}";\n    }\n\nprotected:\n    std::unique_ptr<MemoryTracker> performance_memory_tracker_;\n    std::unique_ptr<PerformanceLab> performance_lab_;\n    std::unique_ptr<ECSPerformanceBenchmarker> ecs_benchmarker_;\n    std::unique_ptr<ECSPerformanceRegressionTester> regression_tester_;\n    std::unique_ptr<memory::MemoryBenchmarkSuite> memory_benchmarker_;\n    std::unique_ptr<ECSPerformanceVisualizer> performance_visualizer_;\n    \n    // Test scenario management\n    struct PerformanceTestScenario {\n        std::string name;\n        std::string description;\n        std::function<bool()> test_function;\n        double timeout_seconds;\n        std::vector<std::string> tags;\n    };\n    \n    std::vector<PerformanceTestScenario> test_scenarios_;\n    std::vector<ECSBenchmarkResult> current_results_;\n    std::vector<ECSBenchmarkResult> baseline_results_;\n    bool has_baseline_data_{false};\n    \n    // Statistics\n    int completed_scenarios_{0};\n    int failed_scenarios_{0};\n    double total_test_time_seconds_{0.0};\n};\n\n// =============================================================================\n// ECS Performance Benchmarking Tests\n// =============================================================================\n\nTEST_F(ComprehensivePerformanceTest, ECSArchitectureComparison) {\n    ecs_benchmarker_->register_all_standard_tests();\n    \n    std::vector<ECSArchitectureType> architectures = {\n        ECSArchitectureType::SparseSet,\n        ECSArchitectureType::Archetype_SoA\n    };\n    \n    ecs_benchmarker_->run_architecture_comparison(architectures);\n    \n    // Wait for completion\n    auto timeout = std::chrono::steady_clock::now() + std::chrono::seconds(60);\n    while (ecs_benchmarker_->is_running() && \n           std::chrono::steady_clock::now() < timeout) {\n        std::this_thread::sleep_for(std::chrono::milliseconds(100));\n    }\n    \n    EXPECT_FALSE(ecs_benchmarker_->is_running());\n    \n    auto results = ecs_benchmarker_->get_results();\n    EXPECT_GT(results.size(), 0);\n    \n    // Analyze architecture performance\n    ecs_benchmarker_->analyze_results();\n    auto comparisons = ecs_benchmarker_->get_architecture_comparisons();\n    \n    EXPECT_EQ(comparisons.size(), architectures.size());\n    \n    // Each architecture should have performance data\n    for (const auto& comparison : comparisons) {\n        EXPECT_GT(comparison.overall_score, 0.0);\n        EXPECT_FALSE(comparison.test_scores.empty());\n        \n        std::cout << "Architecture " \n                  << ECSPerformanceBenchmarker::architecture_to_string(comparison.architecture)\n                  << " overall score: " << comparison.overall_score << "\\n";\n    }\n}\n\nTEST_F(ComprehensivePerformanceTest, ECSScalingPerformance) {\n    std::vector<u32> entity_counts = {100, 500, 1000, 5000, 10000};\n    \n    ecs_benchmarker_->register_all_standard_tests();\n    ecs_benchmarker_->run_scaling_analysis(entity_counts);\n    \n    auto timeout = std::chrono::steady_clock::now() + std::chrono::seconds(120);\n    while (ecs_benchmarker_->is_running() && \n           std::chrono::steady_clock::now() < timeout) {\n        std::this_thread::sleep_for(std::chrono::milliseconds(100));\n    }\n    \n    EXPECT_FALSE(ecs_benchmarker_->is_running());\n    \n    auto results = ecs_benchmarker_->get_results();\n    EXPECT_GT(results.size(), 0);\n    \n    // Analyze scaling characteristics\n    std::string scaling_analysis = ecs_benchmarker_->generate_scaling_analysis();\n    EXPECT_FALSE(scaling_analysis.empty());\n    \n    // Verify scaling data for each entity count\n    for (u32 count : entity_counts) {\n        bool found_result = std::any_of(results.begin(), results.end(),\n            [count](const auto& result) { return result.entity_count == count; });\n        EXPECT_TRUE(found_result) << "Missing result for entity count: " << count;\n    }\n    \n    std::cout << "Scaling Analysis:\\n" << scaling_analysis.substr(0, 500) << "...\\n";\n}\n\nTEST_F(ComprehensivePerformanceTest, PerformanceRegressionDetection) {\n    if (!has_baseline_data_) {\n        GTEST_SKIP() << "No baseline data available for regression testing";\n    }\n    \n    // Run current performance tests\n    ecs_benchmarker_->register_all_standard_tests();\n    ecs_benchmarker_->run_scaling_analysis({1000, 5000});\n    \n    auto timeout = std::chrono::steady_clock::now() + std::chrono::seconds(60);\n    while (ecs_benchmarker_->is_running() && \n           std::chrono::steady_clock::now() < timeout) {\n        std::this_thread::sleep_for(std::chrono::milliseconds(100));\n    }\n    \n    auto current_results = ecs_benchmarker_->get_results();\n    ASSERT_GT(current_results.size(), 0);\n    \n    // Set baseline and detect regressions\n    regression_tester_->set_baseline_results(baseline_results_);\n    auto regression_report = regression_tester_->detect_regressions(current_results);\n    \n    // Report findings\n    std::cout << "Regression Analysis:\\n";\n    std::cout << "  Regressions: " << regression_report.regressions.size() << "\\n";\n    std::cout << "  Improvements: " << regression_report.improvements.size() << "\\n";\n    \n    for (const auto& regression : regression_report.regressions) {\n        std::cout << "  REGRESSION: " << regression.test_name \n                  << " is " << (regression.performance_change * 100.0) << "% slower\\n";\n        \n        // Fail test if regression is significant (> 20%)\n        EXPECT_LT(regression.performance_change, 0.20) \n            << "Significant performance regression in " << regression.test_name;\n    }\n    \n    for (const auto& improvement : regression_report.improvements) {\n        std::cout << "  IMPROVEMENT: " << improvement.test_name \n                  << " is " << (improvement.performance_change * 100.0) << "% faster\\n";\n    }\n}\n\n// =============================================================================\n// Memory Performance Tests\n// =============================================================================\n\nTEST_F(ComprehensivePerformanceTest, MemoryAllocationBenchmarks) {\n    memory_benchmarker_->configure_comprehensive_testing();\n    memory_benchmarker_->run_all_benchmarks();\n    \n    auto results = memory_benchmarker_->get_results();\n    EXPECT_GT(results.size(), 0);\n    \n    auto comprehensive_results = memory_benchmarker_->get_comprehensive_results();\n    \n    // Validate memory benchmark results\n    EXPECT_GT(comprehensive_results.peak_memory_usage, 0);\n    EXPECT_GT(comprehensive_results.total_allocations, 0);\n    EXPECT_GE(comprehensive_results.memory_bandwidth_mb_per_sec, 0.0);\n    \n    std::cout << "Memory Performance Results:\\n";\n    std::cout << "  Peak Usage: " << comprehensive_results.peak_memory_usage << " bytes\\n";\n    std::cout << "  Total Allocations: " << comprehensive_results.total_allocations << "\\n";\n    std::cout << "  Average Allocation Size: " << comprehensive_results.average_allocation_size << " bytes\\n";\n    std::cout << "  Fragmentation Ratio: " << comprehensive_results.fragmentation_ratio << "\\n";\n    std::cout << "  Memory Bandwidth: " << comprehensive_results.memory_bandwidth_mb_per_sec << " MB/s\\n";\n    \n    // Memory bandwidth should be reasonable (at least 100 MB/s)\n    EXPECT_GT(comprehensive_results.memory_bandwidth_mb_per_sec, 100.0);\n    \n    // Fragmentation should be manageable (< 50%)\n    EXPECT_LT(comprehensive_results.fragmentation_ratio, 0.5);\n}\n\nTEST_F(ComprehensivePerformanceTest, MemoryPoolPerformance) {\n    // Test different pool configurations\n    std::vector<std::pair<size_t, size_t>> pool_configs = {\n        {32, 1000},    // Small blocks\n        {128, 500},    // Medium blocks\n        {512, 100},    // Large blocks\n        {1024, 50}     // Very large blocks\n    };\n    \n    for (const auto& [block_size, block_count] : pool_configs) {\n        std::string test_name = "Pool_" + std::to_string(block_size) + "_" + std::to_string(block_count);\n        \n        auto pool_benchmark = memory_benchmarker_->create_pool_benchmark(test_name, block_size, block_count);\n        pool_benchmark->run();\n        \n        auto pool_result = pool_benchmark->get_result();\n        EXPECT_TRUE(pool_result.is_valid);\n        EXPECT_GT(pool_result.allocations_per_second, 0.0);\n        \n        std::cout << "Pool Performance (" << block_size << "B blocks): "\n                  << pool_result.allocations_per_second << " allocs/sec\\n";\n    }\n}\n\n// =============================================================================\n// Comprehensive Integration Performance Tests\n// =============================================================================\n\nTEST_F(ComprehensivePerformanceTest, RunAllPerformanceScenarios) {\n    auto test_start_time = std::chrono::high_resolution_clock::now();\n    \n    for (auto& scenario : test_scenarios_) {\n        std::cout << "Running performance scenario: " << scenario.name << "\\n";\n        \n        auto scenario_start = std::chrono::high_resolution_clock::now();\n        \n        // Run scenario with timeout\n        std::future<bool> scenario_future = std::async(std::launch::async, scenario.test_function);\n        \n        auto timeout_duration = std::chrono::duration<double>(scenario.timeout_seconds);\n        auto status = scenario_future.wait_for(timeout_duration);\n        \n        bool scenario_passed = false;\n        if (status == std::future_status::ready) {\n            scenario_passed = scenario_future.get();\n        } else {\n            std::cout << "  TIMEOUT: Scenario exceeded " << scenario.timeout_seconds << " seconds\\n";\n        }\n        \n        auto scenario_end = std::chrono::high_resolution_clock::now();\n        auto scenario_duration = std::chrono::duration<double>(scenario_end - scenario_start).count();\n        \n        if (scenario_passed) {\n            completed_scenarios_++;\n            std::cout << "  PASSED in " << scenario_duration << " seconds\\n";\n        } else {\n            failed_scenarios_++;\n            std::cout << "  FAILED after " << scenario_duration << " seconds\\n";\n        }\n        \n        total_test_time_seconds_ += scenario_duration;\n        \n        // Don't fail the entire test suite for individual scenario failures\n        // EXPECT_TRUE(scenario_passed) << "Performance scenario failed: " << scenario.name;\n    }\n    \n    auto test_end_time = std::chrono::high_resolution_clock::now();\n    auto total_duration = std::chrono::duration<double>(test_end_time - test_start_time).count();\n    \n    std::cout << "\\nPerformance Test Suite Summary:\\n";\n    std::cout << "  Total Scenarios: " << test_scenarios_.size() << "\\n";\n    std::cout << "  Completed: " << completed_scenarios_ << "\\n";\n    std::cout << "  Failed: " << failed_scenarios_ << "\\n";\n    std::cout << "  Total Time: " << total_duration << " seconds\\n";\n    std::cout << "  Success Rate: " \n              << (static_cast<double>(completed_scenarios_) / test_scenarios_.size() * 100.0) \n              << "%\\n";\n    \n    // At least 80% of scenarios should pass\n    double success_rate = static_cast<double>(completed_scenarios_) / test_scenarios_.size();\n    EXPECT_GT(success_rate, 0.8) << "Performance test suite success rate too low";\n}\n\nTEST_F(ComprehensivePerformanceTest, PerformanceVisualizationData) {\n    // Test performance visualization data generation\n    ecs_benchmarker_->register_all_standard_tests();\n    ecs_benchmarker_->run_scaling_analysis({100, 500, 1000});\n    \n    auto timeout = std::chrono::steady_clock::now() + std::chrono::seconds(30);\n    while (ecs_benchmarker_->is_running() && \n           std::chrono::steady_clock::now() < timeout) {\n        std::this_thread::sleep_for(std::chrono::milliseconds(100));\n    }\n    \n    auto results = ecs_benchmarker_->get_results();\n    ASSERT_GT(results.size(), 0);\n    \n    // Set results in visualizer\n    performance_visualizer_->set_benchmark_results(results);\n    \n    // Test visualization data generation\n    auto chart_data = performance_visualizer_->generate_performance_chart_data();\n    EXPECT_FALSE(chart_data.empty());\n    \n    auto scaling_chart = performance_visualizer_->generate_scaling_chart_data();\n    EXPECT_FALSE(scaling_chart.empty());\n    \n    auto memory_chart = performance_visualizer_->generate_memory_usage_chart();\n    EXPECT_FALSE(memory_chart.empty());\n    \n    // Test data export\n    std::string visualization_data = performance_visualizer_->export_visualization_data();\n    EXPECT_FALSE(visualization_data.empty());\n    EXPECT_GT(visualization_data.length(), 100);\n    \n    std::cout << "Generated visualization data: " << visualization_data.length() << " characters\\n";\n}\n\n// =============================================================================\n// Educational Performance Insights Tests\n// =============================================================================\n\nTEST_F(ComprehensivePerformanceTest, EducationalInsightsGeneration) {\n    // Run some benchmarks to generate data\n    ecs_benchmarker_->register_all_standard_tests();\n    ecs_benchmarker_->run_architecture_comparison({\n        ECSArchitectureType::SparseSet,\n        ECSArchitectureType::Archetype_SoA\n    });\n    \n    auto timeout = std::chrono::steady_clock::now() + std::chrono::seconds(30);\n    while (ecs_benchmarker_->is_running() && \n           std::chrono::steady_clock::now() < timeout) {\n        std::this_thread::sleep_for(std::chrono::milliseconds(100));\n    }\n    \n    auto results = ecs_benchmarker_->get_results();\n    ASSERT_GT(results.size(), 0);\n    \n    // Test educational insights\n    auto insights = ecs_benchmarker_->get_educational_insights();\n    EXPECT_FALSE(insights.empty());\n    \n    for (const auto& insight : insights) {\n        EXPECT_GT(insight.length(), 20); // Each insight should be substantial\n        std::cout << "Educational Insight: " << insight << "\\n";\n    }\n    \n    // Test result explanations\n    for (const auto& result : results) {\n        std::string explanation = ecs_benchmarker_->explain_result(result);\n        EXPECT_FALSE(explanation.empty());\n        \n        auto optimizations = ecs_benchmarker_->suggest_optimizations(result);\n        EXPECT_FALSE(optimizations.empty());\n        \n        std::cout << "\\nResult: " << result.test_name << "\\n";\n        std::cout << "Explanation: " << explanation << "\\n";\n        std::cout << "Optimizations:\\n";\n        for (const auto& opt : optimizations) {\n            std::cout << "  - " << opt << "\\n";\n        }\n    }\n    \n    // Test optimization recommendations\n    std::string recommendations = ecs_benchmarker_->generate_optimization_recommendations();\n    EXPECT_FALSE(recommendations.empty());\n    EXPECT_GT(recommendations.length(), 200);\n    \n    std::cout << "\\nOptimization Recommendations:\\n" << recommendations << "\\n";\n}\n\n} // namespace ECScope::Testing"